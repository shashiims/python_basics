section following the in explained briefly been have Optimization Adam and Descent Gradient Optimization. Adam is technique such One Matrix. Hessian by represented derivative order second include Optimization Order Second Whereas AdaDelta.. Adagrad, momentum, of introduction as such variants the in done have improvements results, improve to order In Descent. Gradient Stochastic and Descent Gradient Batch Mini like Descent Gradient of variants of number a exist There Descent. Gradient is technique used widely the matrix, Jacobian by represented gradient of computation the include Optimization Order First The algorithms. optimization order second and algorithms optimization order first i.e. categories two into branched be can algorithms optimization the Majorly etc. bias weight, like parameters learnable various on dependent E(x)), function loss called (often function objective the minimizing by network the benefit algorithms optimizing called often Algorithms Learning 